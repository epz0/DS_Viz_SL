---
phase: 00-data-preparation
plan: 01
type: execute
wave: 1
depends_on: []
files_modified:
  - scripts/precompute.py
  - streamlit_app/data/.gitkeep
autonomous: true

must_haves:
  truths:
    - "Running `python scripts/precompute.py all` executes the full pipeline from Excel read through novelty and outputs all cached data files"
    - "Running `python scripts/precompute.py read` (or any other step) executes only that step independently"
    - "Steps whose output already exists are skipped unless --force is passed"
    - "Pipeline fails fast on any error without producing partial output in subsequent steps"
  artifacts:
    - path: "scripts/precompute.py"
      provides: "CLI pipeline script with argparse subcommands for all pipeline steps"
      min_lines: 300
    - path: "streamlit_app/data/.gitkeep"
      provides: "Output directory for pre-computed data files"
  key_links:
    - from: "scripts/precompute.py"
      to: "scripts/design_space/read_data.py"
      via: "import read_analysis"
      pattern: "from scripts\\.design_space\\.read_data import read_analysis"
    - from: "scripts/precompute.py"
      to: "scripts/design_space/dist_matrix.py"
      via: "import calc_distmatrix, create_dmatrix_from_embed"
      pattern: "from scripts\\.design_space\\.dist_matrix import"
    - from: "scripts/precompute.py"
      to: "scripts/design_space/dim_reduction.py"
      via: "import create_embedding"
      pattern: "from scripts\\.design_space\\.dim_reduction import create_embedding"
    - from: "scripts/precompute.py"
      to: "scripts/utils/utils.py"
      via: "import unmask_data, solutions_summary, cv_hull_vertices"
      pattern: "from scripts\\.utils\\.utils import"
    - from: "scripts/precompute.py"
      to: "streamlit_app/data/"
      via: "writes parquet, pickle, JSON output files"
      pattern: "OUTPUT_DIR"
---

<objective>
Build the pre-computation CLI pipeline script that runs all heavy computation steps (Excel read, distance matrix, UMAP embedding, unmask, metrics, clustering, novelty, convex hulls) and outputs cached data files to `streamlit_app/data/`.

Purpose: This is the core of Phase 0 — extracting all computation logic from the Dash app (`scripts/interactive_tool.py`) into a modular, reusable pipeline that produces the cached data the Streamlit app will load.

Output: `scripts/precompute.py` with argparse subcommands, `streamlit_app/data/` directory created.
</objective>

<execution_context>
@C:/Users/e_par/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/e_par/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/00-data-preparation/00-CONTEXT.md
@.planning/phases/00-data-preparation/00-RESEARCH.md

# Source files containing computation logic to extract:
@scripts/interactive_tool.py
@scripts/interactive_run.py
@scripts/design_space/read_data.py
@scripts/design_space/dist_matrix.py
@scripts/design_space/dim_reduction.py
@scripts/design_space/dspace_dist_metrics.py
@scripts/design_space/design_space.py
@scripts/design_space/dspace_metrics.py
@scripts/design_space/dspace_metric_novelty.py
@scripts/design_space/dspace_viz_density.py
@scripts/design_space/dspace_cluster.py
@scripts/utils/utils.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Create precompute.py pipeline script with all computation steps</name>
  <files>scripts/precompute.py, streamlit_app/data/.gitkeep</files>
  <action>
Create `streamlit_app/data/` directory with a `.gitkeep` file.

Create `scripts/precompute.py` — a single-file CLI pipeline script that extracts all computation logic from `scripts/interactive_tool.py` (lines 1-220) and orchestrates it via argparse subcommands.

**CLI structure** (per user decision: modular steps + "all" command):
- `python scripts/precompute.py all [--force]` — runs full pipeline
- `python scripts/precompute.py read [--force]` — read Excel
- `python scripts/precompute.py distance [--force]` — distance matrix
- `python scripts/precompute.py umap [--force]` — UMAP embedding
- `python scripts/precompute.py unmask [--force]` — unmask participant data
- `python scripts/precompute.py features [--force]` — core attributes, solution summary, hover text, cluster symbols, performance
- `python scripts/precompute.py metrics [--force]` — distance metrics, area metrics
- `python scripts/precompute.py clusters [--force]` — Leiden clustering + cluster summary
- `python scripts/precompute.py novelty [--force]` — novelty from density + neighbors
- `python scripts/precompute.py hulls [--force]` — convex hull vertices per participant + full DS
- `python scripts/precompute.py validate` — validate all outputs (always runs, no skip)

**Path handling** (per research pitfall #5):
- Use `SCRIPT_DIR = Path(__file__).resolve().parent` for absolute path from script location
- `PROJECT_ROOT = SCRIPT_DIR.parent`
- `DATA_DIR = PROJECT_ROOT / 'data'`
- `OUTPUT_DIR = PROJECT_ROOT / 'streamlit_app' / 'data'`
- Hardcoded filename: `FILENAME = 'MASKED_DATA_analysis_v2.xlsx'` (per user decision)
- Sheet name: `SHEET = 'ExpData-100R-Expanded'`

**Imports** — use the existing modules in `scripts/design_space/` and `scripts/utils/`:
- The script must add `PROJECT_ROOT` to `sys.path` so imports like `from scripts.design_space.read_data import read_analysis` work regardless of cwd.

**Logging** (per research recommendation):
- Use `logging` module with module-level logger
- Configure in `if __name__ == '__main__'` block with format: `'%(asctime)s - %(levelname)s - %(message)s'`
- Log step start, skip (if output exists), completion, and errors

**Skip-if-exists logic** (per user decision):
- Each step checks if its output file(s) exist before running
- If exists and not `--force`: log skip message and return the loaded data (needed for downstream steps)
- If exists and `--force`: log recompute message and proceed
- Validate step never skips

**Fail-fast** (per user decision):
- No try/except around pipeline steps in `run_all_steps` — any exception propagates immediately
- Each individual step function should NOT catch exceptions (let them propagate)

**Pipeline step implementations** — extract from `interactive_tool.py`:

1. **`step_read(force)`**: Call `read_analysis(DATA_DIR, FILENAME, sheetname=SHEET)`. Save df_base as pickle (not parquet yet — it has no computed columns at this stage and may have object types). Save df_colors as pickle. Return df_base, df_colors, labels.
   - Output: `OUTPUT_DIR / 'intermediate' / 'df_read.pkl'`, `OUTPUT_DIR / 'intermediate' / 'df_colors.pkl'`

2. **`step_distance(force)`**: Load df_base from step_read output. Call `calc_distmatrix(df_base, DATA_DIR, FILENAME)`. Save n_distmatrix as numpy file. Return n_distmatrix.
   - Output: `OUTPUT_DIR / 'intermediate' / 'n_distmatrix.npy'`

3. **`step_umap(force)`**: Load n_distmatrix. Call `create_embedding(DATA_DIR, n_distmatrix, Wg='W2', NN=115, MD=0.15, densm=2)`. Save embedding as numpy file. Save graph as pickle (needed for clustering). Return embedding, graph.
   - Output: `OUTPUT_DIR / 'intermediate' / 'embedding.npy'`, `OUTPUT_DIR / 'intermediate' / 'graph.pkl'`
   - IMPORTANT: Use pickle protocol 5 for the graph object. Do NOT try to pickle the UMAP model — only save the embedding array and the igraph graph.

4. **`step_unmask(force)`**: Load df_base, df_colors. Call `unmask_data(DATA_DIR, 'MASKING_KEYS', df_base, df_colors)` — note this function returns (df_base, df_colors) when df_colors is passed (see interactive_tool.py line 52). Save updated df_base and df_colors. Return df_base, df_colors.
   - Output: `OUTPUT_DIR / 'intermediate' / 'df_unmask.pkl'`, `OUTPUT_DIR / 'intermediate' / 'df_colors_unmask.pkl'`

5. **`step_features(force)`**: Load df_base (unmasked), df_colors, embedding. Add computed columns exactly as in interactive_tool.py lines 55-131:
   - `x_emb`, `y_emb` from embedding
   - Merge with df_colors on ParticipantID/P for HEX-Win
   - Handle NaN OriginalID_PT/Group/Sol/PrePost (set to 'GALL' or SolutionID)
   - Create `ids` = sorted unique OriginalID_PT
   - Core attributes: `ca_sol`, `ca_deck`, `ca_str`, `ca_rck`, `ca_road`, `ca_rroad`, `ca_wood`, `ca_steel`, `ca_mtr`, `ca_perf`, `performance`, `fullid_orig`
   - Merge with `solutions_summary()` for TLength, NSegm, NJoint — note `solutions_summary` takes `save_dir` param pointing to `data/json` directory. Check the function signature in utils.py to handle the save_dir path correctly (it should be `DATA_DIR / 'json'`).
   - Handle NaN for TLength, NSegm, NJoint (set to 'N/A')
   - Save updated df_base. Return df_base.
   - Output: `OUTPUT_DIR / 'intermediate' / 'df_features.pkl'`

6. **`step_metrics(force)`**: Load df_base (with features), embedding. Compute distance and area metrics exactly as in interactive_tool.py lines 133-175:
   - Distance metrics: `create_dmatrix_from_embed(DATA_DIR, embedding, norm=False)`, then `dist_metrics_fs`, `dist_metrics_pre`, `dist_metrics_post`. Drop specified columns, merge.
   - Area metrics: `create_cvxh(DATA_DIR, df_base, embedding, df_colors, save_plot=False)`. Then `area_summary(df_base, DS_area, df_metrics_fs, mode='all', df_ch_pre_metrics=df_metrics_pre, df_ch_post_metrics=df_metrics_post)`. Drop columns, merge, rename Area columns.
   - Handle NaN for all metric columns (set to 'N/A').
   - Save DS_Area value for metadata.
   - Output: `OUTPUT_DIR / 'intermediate' / 'df_metrics.pkl'`, `OUTPUT_DIR / 'intermediate' / 'ds_area.json'`

7. **`step_clusters(force)`**: Load df_base (with metrics), graph. Compute clusters exactly as in interactive_tool.py lines 196-218:
   - `get_clusters(df_base, graph)` — adds `cluster_id` column
   - Create cluster symbol mapping: `symb_ls` list as defined on line 199-203, merge with df_base
   - Compute per-participant cluster counts: `n_clusters`, `n_clusters_pre`, `n_clusters_post` (loop over ids[1:], exactly as lines 207-218)
   - Create `hovertxt` column (line 220)
   - Output: `OUTPUT_DIR / 'intermediate' / 'df_clusters.pkl'`

8. **`step_novelty(force)`**: Load df_base (with clusters), embedding. Compute novelty exactly as in interactive_tool.py lines 178-193:
   - Density novelty: `prep_density(df_base, embedding)`, then `novelty_from_density(DATA_DIR, df_kde, lim_x, lim_y, prt_integral=False, save_metrics=False)`. Drop columns, merge.
   - Neighbor novelty: `novelty_from_neig(DATA_DIR, df_base, embedding, delta=0.9)` — NOTE: interactive_tool.py uses delta=0.9 (not 0.7 as in research doc — use 0.9 to match the actual Dash app). Drop columns, merge.
   - Output: `OUTPUT_DIR / 'intermediate' / 'df_novelty.pkl'`

9. **`step_hulls(force)`**: Load final df_base. Compute convex hull vertices for each participant and the full DS:
   - Full DS hull: `cv_hull_vertices(df_base['x_emb'], df_base['y_emb'])` — save vertices and area
   - Per-participant hulls: For each unique OriginalID_PT, compute hull vertices using shapely (as in interactive_tool.py lines 255-278 which uses `shapely.geometry.MultiPoint(...).convex_hull.exterior.coords`)
   - Store as dict: `{participant_id: {'x': [...], 'y': [...], 'area': float}, 'full_ds': {'x': [...], 'y': [...], 'area': float}}`
   - Output: `OUTPUT_DIR / 'convex_hulls.pkl'` (pickle protocol 5)

10. **`step_save_final(force)`**: Load final df_base (with all computed columns). Save final outputs:
    - `OUTPUT_DIR / 'df_base.parquet'` — convert any list/dict columns to JSON strings first to avoid Arrow errors. If parquet conversion fails due to mixed types, fall back to pickle.
    - `OUTPUT_DIR / 'metadata.json'` — participant IDs, color mapping (OriginalID_PT -> HEX-Win), DS area, cluster symbol mapping, ids list (the sorted OriginalID_PT list used for dropdown)
    - `OUTPUT_DIR / 'manifest.json'` — timestamp, source file, UMAP params (NN=115, MD=0.15, densm=2), cluster resolution (0.05), novelty delta (0.9), list of output files

11. **`run_all_steps(force)`**: Calls steps 1-10 in order, passing data between steps in memory (not re-reading from disk). Auto-calls validate at the end (per user decision).

**The `all` command MUST pass data in memory** between steps when run sequentially. Each step returns its result AND saves to disk. When running individually, steps load required inputs from disk.

For individual step execution: each step function should first try to load its prerequisites from the intermediate directory. If prerequisites don't exist, raise a clear error: "Step X requires output from step Y. Run `python scripts/precompute.py Y` first or use `all`."

Create `OUTPUT_DIR / 'intermediate'` subdirectory for intermediate pickle files. Only final outputs (df_base.parquet, convex_hulls.pkl, metadata.json, manifest.json) go directly in `OUTPUT_DIR`.
  </action>
  <verify>
  Run `python scripts/precompute.py --help` and verify all subcommands appear.
  Run `python scripts/precompute.py read --help` and verify --force flag appears.
  Verify `streamlit_app/data/.gitkeep` exists.
  Verify the script has no syntax errors: `python -c "import scripts.precompute"` (may need sys.path adjustment).
  </verify>
  <done>
  `scripts/precompute.py` exists with all pipeline steps implemented, argparse CLI with subcommands for each step plus `all`, skip-if-exists with --force override, fail-fast error handling, logging, and output to `streamlit_app/data/`. The script can be imported without errors and `--help` shows all expected subcommands.
  </done>
</task>

</tasks>

<verification>
- `python scripts/precompute.py --help` shows all subcommands (read, distance, umap, unmask, features, metrics, clusters, novelty, hulls, validate, all)
- `python scripts/precompute.py all --help` shows --force flag
- Script imports resolve without errors
- `streamlit_app/data/` directory exists
</verification>

<success_criteria>
- precompute.py exists with ~300+ lines implementing all pipeline steps
- CLI structure matches user decision (modular steps + all command)
- All computation logic extracted from interactive_tool.py
- Skip-if-exists with --force override implemented
- Fail-fast error handling (no silent catches)
- Output directory structure created
</success_criteria>

<output>
After completion, create `.planning/phases/00-data-preparation/00-01-SUMMARY.md`
</output>
