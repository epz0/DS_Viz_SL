---
phase: 00-data-preparation
plan: 02
type: execute
wave: 2
depends_on: ["00-01"]
files_modified:
  - scripts/precompute.py
  - requirements-dev.txt
autonomous: false

must_haves:
  truths:
    - "Running `python scripts/precompute.py all` produces all output files in streamlit_app/data/ without errors"
    - "Validation step confirms all 80+ columns required by the visualization are present in df_base.parquet"
    - "requirements-dev.txt includes all dependencies needed to run the pipeline"
    - "Existing scripts in scripts/ directory remain functional (no imports broken)"
  artifacts:
    - path: "streamlit_app/data/df_base.parquet"
      provides: "Main DataFrame with all 80+ computed columns"
    - path: "streamlit_app/data/convex_hulls.pkl"
      provides: "Convex hull vertices per participant and full DS"
    - path: "streamlit_app/data/metadata.json"
      provides: "Participant IDs, color mapping, DS area, cluster symbols"
    - path: "streamlit_app/data/manifest.json"
      provides: "Computation metadata and provenance"
    - path: "requirements-dev.txt"
      provides: "Full pipeline dependencies including -r requirements.txt"
  key_links:
    - from: "scripts/precompute.py validate"
      to: "streamlit_app/data/df_base.parquet"
      via: "reads and validates column presence and types"
      pattern: "validate"
    - from: "requirements-dev.txt"
      to: "requirements.txt"
      via: "-r requirements.txt include"
      pattern: "-r requirements.txt"
---

<objective>
Add the validation step to precompute.py, create requirements-dev.txt, run the full pipeline end-to-end, and verify all outputs are correct.

Purpose: This plan completes Phase 0 by ensuring the pipeline actually runs, produces valid output, and the dependency management is in place for both local development and Streamlit deployment.

Output: All pre-computed data files in `streamlit_app/data/`, validated validation step, `requirements-dev.txt`.
</objective>

<execution_context>
@C:/Users/e_par/.claude/get-shit-done/workflows/execute-plan.md
@C:/Users/e_par/.claude/get-shit-done/templates/summary.md
</execution_context>

<context>
@.planning/PROJECT.md
@.planning/ROADMAP.md
@.planning/STATE.md
@.planning/phases/00-data-preparation/00-CONTEXT.md
@.planning/phases/00-data-preparation/00-RESEARCH.md
@.planning/phases/00-data-preparation/00-01-SUMMARY.md

# The script created in Plan 01:
@scripts/precompute.py
@scripts/interactive_tool.py
</context>

<tasks>

<task type="auto">
  <name>Task 1: Implement validation step and create requirements-dev.txt</name>
  <files>scripts/precompute.py, requirements-dev.txt</files>
  <action>
**1. Add validation step to `scripts/precompute.py`** (the `step_validate()` function):

The validate step checks all final output files and confirms data integrity. It should NOT use pandera (to avoid adding a dependency) — use plain pandas checks instead.

Validation checks (per user decision: auto-validate after every run, check all 80+ columns):

a) **File existence**: Check that all four final output files exist:
   - `streamlit_app/data/df_base.parquet`
   - `streamlit_app/data/convex_hulls.pkl`
   - `streamlit_app/data/metadata.json`
   - `streamlit_app/data/manifest.json`

b) **df_base column validation**: Load parquet, verify ALL of these columns are present (extracted from interactive_tool.py callbacks — these are the columns the Streamlit app will need):
   - Identity: `FullID`, `ParticipantID`, `SolutionID`, `GroupID`
   - Embedding: `x_emb`, `y_emb`
   - Original IDs (from unmask): `OriginalID_PT`, `OriginalID_Group`, `OriginalID_Sol`, `OriginalID_PrePost`
   - Result/cost: `result`, `budgetUsed`, `maxStress`
   - Core attributes: `ca_sol`, `ca_deck`, `ca_str`, `ca_rck`, `ca_mtr`, `ca_perf`, `performance`
   - Solution summary: `TLength`, `NSegm`, `NJoint`
   - Visual: `HEX-Win`, `hovertxt`, `clust_symb`, `videoPreview`
   - Full ID: `fullid_orig`
   - Clustering: `cluster_id`, `n_clusters`, `n_clusters_pre`, `n_clusters_post`
   - Distance metrics: `totaldist_FS`, `totaldist_PRE`, `totaldist_PST`
   - Area metrics: `Area-Perc-FS`, `Area-Perc-PRE`, `Area-Perc-POST`
   - Novelty: `novel_nn`, `novelty_norm`
   - Design attributes (from original Excel — needed for distance matrix but also present in df_base): `type`, `numAnchorsUsed`, `deckType_1`, `deckType_2`, `deckShape_1`, `deckShape_2`, `deckShape_3`, `deckShape_4`, `structurePosition_Top`, `structurePosition_Rock`, `structurePosition_Bottom`, `structureShape_1`, `structureShape_2`, `structureSize`, `rockSupportShape`, `rockSupportMat`, `materialRoad`, `materialReinfRoad`, `materialWood`, `materialSteel`
   - Note: There may be additional columns from the Excel source — the check should verify the REQUIRED columns above are present but allow extra columns.

c) **Row count validation**: df_base should have >0 rows. Log the count.

d) **Convex hulls validation**: Load pickle, verify it's a dict, has 'full_ds' key, and has at least 30 participant keys (there are 31 participants including gallery).

e) **Metadata validation**: Load JSON, verify keys: `participant_ids` (list, non-empty), `color_mapping` (dict, non-empty), `ds_area` (float, >0), `cluster_symbols` (dict or list), `ids` (list).

f) **Validation output**: Log all checks with pass/fail. If ANY check fails, raise a ValueError with a summary of failures. Per user decision: fail fast. Keep the output files for debugging (don't delete them).

**2. Create `requirements-dev.txt`**:

```
# Development dependencies (full pipeline)
# Includes production dependencies
-r requirements.txt

# Additional pipeline dependencies not needed in production
# (most are already in requirements.txt from the Dash app era,
#  but listing explicitly for clarity)
```

The current requirements.txt already has ALL the heavy dependencies (umap-learn, scipy, scikit-learn, igraph, gower, etc.) because it was the Dash app's requirements. For now, requirements-dev.txt just includes `-r requirements.txt`. In Phase 1, when we create a slim requirements.txt for Streamlit deployment, the heavy deps will move to requirements-dev.txt only.

Do NOT modify the existing requirements.txt in this phase — it will be slimmed down in Phase 1.
  </action>
  <verify>
  Verify `requirements-dev.txt` exists and contains `-r requirements.txt`.
  Verify `scripts/precompute.py` has a `step_validate` function that checks file existence, column presence, row counts, and data integrity.
  </verify>
  <done>
  Validation step implemented in precompute.py with comprehensive checks for all required columns and data integrity. requirements-dev.txt created with -r requirements.txt include.
  </done>
</task>

<task type="auto">
  <name>Task 2: Run full pipeline and fix any issues</name>
  <files>scripts/precompute.py</files>
  <action>
Run the full pre-computation pipeline:

```bash
cd C:/Py/DS_Viz_SL
python scripts/precompute.py all
```

This will:
1. Read the Excel file
2. Compute distance matrix
3. Generate UMAP embedding
4. Unmask participant data
5. Compute features (core attributes, solution summary)
6. Compute metrics (distance, area)
7. Run Leiden clustering
8. Compute novelty scores
9. Compute convex hull vertices
10. Save final outputs
11. Auto-validate

**Expected runtime:** 3-10 minutes total (UMAP is the slowest step at ~2 min).

**If errors occur:** Fix the issues in `scripts/precompute.py` and re-run with `--force` to recompute from scratch.

**Common issues to watch for:**
- Import path issues: The script needs PROJECT_ROOT on sys.path
- The existing modules may use `print()` extensively and write to `export/` directory — this is fine, let them do their thing
- `unmask_data()` function signature may differ from what's expected — check if it returns (df, df_colors) or just df depending on whether df_colors is passed
- `solutions_summary()` needs a directory path — ensure it points to `data/json`
- Parquet conversion may fail on object-type columns with mixed types — convert problematic columns to strings first
- The `create_cvxh` function may try to save plots — pass `save_plot=False`
- `novelty_from_density` and `novelty_from_neig` may try to save files — pass `save_df=False` / `save_metrics=False` / `save_plot=False`

After successful run, verify output files:
```bash
ls -la streamlit_app/data/
ls -la streamlit_app/data/intermediate/
```

Expected final outputs:
- `streamlit_app/data/df_base.parquet` (main DataFrame)
- `streamlit_app/data/convex_hulls.pkl` (hull vertices)
- `streamlit_app/data/metadata.json` (participant metadata)
- `streamlit_app/data/manifest.json` (computation provenance)
  </action>
  <verify>
  `python scripts/precompute.py validate` passes all checks.
  All four final output files exist in `streamlit_app/data/`.
  Intermediate files exist in `streamlit_app/data/intermediate/`.
  </verify>
  <done>
  Full pipeline runs without errors, all output files exist, validation passes confirming all 80+ required columns are present in df_base.parquet.
  </done>
</task>

<task type="checkpoint:human-verify" gate="blocking">
  <name>Task 3: Verify pipeline outputs and existing script compatibility</name>
  <files>streamlit_app/data/df_base.parquet, streamlit_app/data/convex_hulls.pkl, streamlit_app/data/metadata.json, streamlit_app/data/manifest.json</files>
  <action>
  Present the following verification steps to the user. What was built: Complete pre-computation pipeline that reads the source Excel, runs all computation steps (distance matrix, UMAP, unmask, features, metrics, clustering, novelty, convex hulls), and outputs cached data files to streamlit_app/data/.
  </action>
  <verify>
  1. Check that `streamlit_app/data/` contains: df_base.parquet, convex_hulls.pkl, metadata.json, manifest.json
  2. Run `python scripts/precompute.py validate` — should pass all checks
  3. Run `python scripts/precompute.py all` (without --force) — should skip all steps (output exists)
  4. Run `python scripts/precompute.py all --force` — should recompute everything
  5. Verify existing scripts still work: open `scripts/interactive_tool.py` in your IDE and confirm it can still run (no broken imports)
  6. Open `streamlit_app/data/metadata.json` and spot-check participant IDs and colors look correct
  </verify>
  <done>User confirms pipeline outputs are correct, skip/force behavior works, and existing scripts are unaffected.</done>
</task>

</tasks>

<verification>
- `python scripts/precompute.py all` completes without errors
- `python scripts/precompute.py validate` passes all checks
- All four final output files exist in `streamlit_app/data/`
- `requirements-dev.txt` exists with `-r requirements.txt`
- Existing scripts in `scripts/` are unmodified and functional
- Running with `--force` recomputes; running without skips existing outputs
</verification>

<success_criteria>
- Pipeline runs end-to-end producing all cached data files
- Validation confirms all 80+ required columns present
- requirements-dev.txt created
- Existing analysis scripts remain functional
- Output files are committed to repo (per user decision: git is the backup)
</success_criteria>

<output>
After completion, create `.planning/phases/00-data-preparation/00-02-SUMMARY.md`
</output>
